import pandas as pd
import numpy as np
import jd

import spacy

import matplotlib.pyplot as plt
import seaborn as sns


import ptitprince as pt

import warnings
warnings.filterwarnings("ignore")


def load_prepare_data(path):
  """
  Función para cargar y procesar datos para el ejercicio.
  """
  df = pd.read_csv(path,sep=",")
  map_classes = {
    "religion":1,
    "age":1,
    "ethnicity":1,
    "gender":1,
    "other_cyberbullying":1,
    "not_cyberbullying":0,
  }
  df["cyberbullying"] = df.cyberbullying_type.map(map_classes)
  return df[["tweet_text","cyberbullying"]].copy()





path_data = "https://raw.githubusercontent.com/luisgasco/ntic_master_datos/main/datasets/cyberbullying_tweets.csv"
dataset = load_prepare_data(path_data)


dataset.head(4)


corpus = dataset["tweet_text"].to_list()
print(len(corpus),type(corpus))



#DATASET RANDOMUNDERSAMPLED: SERÁ USADO EN EL ANÁLISIS PARA HACER COMPARACIONES JUSTAS EN LA APARICIÓN DE CIETTAS PALABRAS:
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=123)
X_res, y_res = rus.fit_resample(dataset[["tweet_text"]], dataset.cyberbullying)
dataset_rus = pd.merge(X_res, y_res,left_index=True,right_index=True)








print("Existen {} documentos duplicadas".format(np.sum(dataset.duplicated(subset=["tweet_text"]))))
tweets_duplicados = dataset[dataset.duplicated(subset=["tweet_text"])]["tweet_text"].tolist()

# [print(x) for x in tweets_duplicados] Parecen ser duplicados por error, no tweets idénticos escritos por dos o más personas.

# Estos duplicados pueden sesgar mi análisis y al desghacerme de ellos lograré que el procesado sea algo más rápido
dataset = dataset.drop_duplicates()


#No hay NAs de los que deshacerse
dataset.tweet_text.isna().sum()





#!python -m spacy download en_core_web_sm
nlp=spacy.load('en_core_web_sm' )


"""El tqdm no me funcionó
Error displaying widget
from tqdm.auto import tqdm
dataset["tweet_text_doc"]= [doc for doc in tqdm(nlp.pipe(dataset["tweet_text"].to_list()), total=len(corpus))]
"""





import tqdm
import contractions
def count_slashes(ser,que_slash = "\r\n"):
    contador=0    
    try:
        for word in ser.split(" "):
            if que_slash in str(word):
                contador +=1
        return contador
    except AttributeError:
        return contador
"""
Retorno carro y newline no aparecen sin estar juntos.

dataset["cantidad_retorno_carro"] = dataset.tweet_text.apply(lambda x: count_slashes(x, que_slash="\r"))
dataset["cantidad_newline"] = dataset.tweet_text.apply(lambda x: count_slashes(x,que_slash="\n"))
dataset[["cantidad_retorno_carro","cantidad_retorno_carro_newline","cantidad_newline"]].describe()
"""

dataset["cantidad_retorno_carro_newline"] = dataset.tweet_text.apply(lambda x: count_slashes(x, que_slash="\r\n"))
dataset["tweet_text"] = [x.replace("\r\n"," ") for x in dataset.tweet_text]
dataset["tweet_text"] = [contractions.fix(x, slang=True) for x in dataset.tweet_text] ## Arreglo slang y contractions que queden

#dataset["tweet_text"] = [[ii.replace("\r\n"," ") for ii in x] for x in dataset.tweet_text] # me deshago de los retorno de carro y new line una vez pasados a características

dataset["tweet_text_doc"] = [doc for doc in nlp.pipe(dataset["tweet_text"].to_list())]





nlp_docs = dataset["tweet_text_doc"].tolist()
print(len(nlp_docs),type(nlp_docs))





plt.figure(figsize=(30,15))

count_dic = {}
for idx,sentence in enumerate(nlp_docs):
    count_dic[idx] = len(sentence)

#print(pd.Series(count_dic).value_counts().sort_values(ascending=False))
pd.Series(count_dic).value_counts().sort_index(ascending=False).plot(kind="barh")
plt.title("Frecuencia de cantidad de palabras por tweet",size=20)
plt.xlabel("Frecuencia",size = 15)
plt.ylabel("Cantidad de palabras",size = 15)





lis_long_tweets = dataset.loc[dataset["tweet_text"].str.len() >1000,"tweet_text"].tolist()
[x for x in lis_long_tweets]#\r\n parece ser un tweet diferente
lis_long_tweets[0]





fig,ax = plt.subplots(figsize=(15,6))
etiquetas = dataset.cyberbullying.value_counts()
etiquetas.plot(kind= 'bar', color= ["red", "blue"])
ax.set_xticklabels(('cyberbullying', 'Not cyberbullying '),rotation=0)
plt.title('Distribución de la variable objetivo')

plt.show()
print("Este desbalance nos traerá problemas a la hora de levantar un modelo predictivo funcional")





import textacy
#import contractions

lista_nlp_tweets_SinStop_lemma = [[ii for ii in x if not ii.is_stop] for x in nlp_docs] ## Saco stopwords
lista_nlp_tweets_SinStop_lemma = [[ii for ii in x if not ii.is_punct] for x in lista_nlp_tweets_SinStop_lemma] ## Saco signos de puntuación
lista_nlp_tweets_SinStop_lemma = [[ii.lemma_ for ii in x] for x in lista_nlp_tweets_SinStop_lemma] ##ii.lema sin _ da números
lista_nlp_tweets_SinStop_lemma = [[ii.lower() for ii in x] for x in lista_nlp_tweets_SinStop_lemma] ## Dejo todo en minúscula
lista_nlp_tweets_SinStop_lemma = [[ii for ii in x if not ii == " "] for x in lista_nlp_tweets_SinStop_lemma] ## Saco los espacios


#Anteriormente arrega¡laba aquí las contractions, pero probablemente sea mejor hacerlo antes de pasar por el pipe de spacy
#lista_nlp_tweets_SinStop_lemma = [[contractions.fix(ii, slang=True) for ii in x] for x in lista_nlp_tweets_SinStop_lemma] ## Arreglo slang y contractions que queden

def is_url(token):
    return token.startswith("https://") or token.startswith("http://") or token.startswith("www.")
lista_nlp_tweets_SinStop_lemma = [["URL" if is_url(ii) else ii for ii in x] for x in lista_nlp_tweets_SinStop_lemma]

                                  
lista_nlp_tweets_SinStop_lemma = [["MENTION" if ii.startswith("@") else ii for ii in x] for x in lista_nlp_tweets_SinStop_lemma] ## Normalizo @s
lista_nlp_tweets_SinStop_lemma = [["HASHTAG" if ii.startswith("#") else ii for ii in x] for x in lista_nlp_tweets_SinStop_lemma] ## Normalizo @s

lista_nlp_tweets_SinStop_lemma = [["NUMBER" if ii.isdigit() else ii for ii in x] for x in lista_nlp_tweets_SinStop_lemma] ## Normalizo números

dataset["tweet_lemma_NoStopwordNoPunct"] = lista_nlp_tweets_SinStop_lemma
dataset["tweet_lemma_NoStopwordNoPunct"] = dataset["tweet_lemma_NoStopwordNoPunct"].astype("O")
dataset["tweet_lemma_NoStopwordNoPunct"] = dataset["tweet_lemma_NoStopwordNoPunct"].apply(lambda x: str(x).replace("[","").replace("]","").replace("'",""))
dataset["tweet_lemma_NoStopwordNoPunct"] = dataset["tweet_lemma_NoStopwordNoPunct"].astype("O")
lista_nlp_tweets_SinStop_lemma = [ii for sublist in lista_nlp_tweets_SinStop_lemma for ii in sublist]
dict_words = {}
for idx, val in enumerate(lista_nlp_tweets_SinStop_lemma):
    dict_words[idx] = val

ser_words = pd.Series(dict_words)

#plt.xticks(rotation=90)

"""
#print(lista_nlp_tweets_SinStop_lemma)
#ser_freq.value_counts()
"""


#ser_words.value_counts().plot(kind="barh")
ax, fig = plt.subplots(figsize=(20,14))
ser_words.value_counts().head(40)
ser_words.value_counts(ascending=False).head(60).plot(kind="barh")
plt.title("lemmas de palabras sin stopwords ni signos de puntuación más presentes en los tweets y su frecuencia",size=20)
plt.xlabel("Frecuencia",size = 15)
plt.ylabel("Lemmas",size = 15)






# Importamos las librerías matplotlib y seaborn:
dataset["char_len"] = dataset["tweet_text"].apply(lambda x: len(x))



ax, fig = plt.subplots(figsize=(20,6))
sns.set_style("darkgrid")

sns.histplot(data=dataset.loc[dataset["tweet_text"].str.len() <300,], x="char_len", hue="cyberbullying", kde=True, legend=True)
plt.legend(["Bullying", "Not Bullying"])
# Definimos el título de los ejes:
plt.xlabel('Caracteres', fontsize=16)
plt.ylabel('Densidad', fontsize=16)





f, ax = plt.subplots(figsize=(20, 5))
pt.RainCloud(x = "cyberbullying", y ="char_len", data = dataset.loc[dataset["tweet_text"].str.len() <160,],
             width_viol = .7, orient = "h" , alpha = .9, dodge = False, pointplot = False)









from spellchecker import SpellChecker


spell = SpellChecker()
x = dataset.tweet_text[0].split(" ")
spell.unknown(x)


spell = SpellChecker()

def amount_misspelled(ser,amount):
    wordlist=ser.split()
    
    amount_miss = len(list(spell.unknown(wordlist)))
    percentage_misspelled= amount_miss/len(wordlist)
    if amount:
        return amount_miss
    else:
        return percentage_misspelled
   
dataset["amount_misspelled_words"] = dataset.tweet_text.apply(lambda x: amount_misspelled(x,amount=True))
dataset["percentage_misspelled_words"] = dataset.tweet_text.apply(lambda x: amount_misspelled(x,amount=False))


dataset.tweet_text.apply(lambda x: amount_misspelled(x,True))


dataset["amount_misspelled_words"].describe()


f, ax = plt.subplots(figsize=(20, 5))
sns.violinplot(x="amount_misspelled_words",y="cyberbullying",data= dataset,orient="h")


f, ax = plt.subplots(figsize=(20, 5))
sns.violinplot(x="percentage_misspelled_words",y="cyberbullying",data= dataset,orient="h")





from wordcloud import WordCloud


ax, fig = plt.subplots(figsize=(30,10))
wordcloud = WordCloud()
txt_cat0 = ",".join(dataset[dataset.cyberbullying==0].tweet_lemma_NoStopwordNoPunct.to_list())
plt.imshow(wordcloud.generate("".join(txt_cat0)))



ax, fig = plt.subplots(figsize=(30,10))
wordcloud = WordCloud()
txt_cat1 = ",".join(dataset[dataset.cyberbullying==1].tweet_lemma_NoStopwordNoPunct.to_list())
plt.imshow(wordcloud.generate("".join(txt_cat1)))






from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=123)
X_res, y_res = rus.fit_resample(dataset.drop("cyberbullying",axis=1), dataset.cyberbullying)
dataset_rus = pd.merge(X_res, y_res,left_index=True,right_index=True)


dataset_contains_MENTION = dataset_rus.loc[dataset_rus["tweet_lemma_NoStopwordNoPunct"].str.contains("MENTION")]
dataset_Notcontains_MENTION= dataset_rus.loc[~dataset_rus["tweet_lemma_NoStopwordNoPunct"].str.contains("MENTION")]

fig, ax = plt.subplots(1, 2, figsize=(15, 6))


dataset_contains_MENTION = dataset_contains_MENTION.cyberbullying.value_counts()
dataset_contains_MENTION.plot(kind='bar', color=["red", "blue"], ax=ax[0])
ax[0].set_xticklabels(('cyberbullying', 'Not cyberbullying'), rotation=0)
ax[0].set_title('Contains "MENTION"')

dataset_Notcontains_MENTION = dataset_Notcontains_MENTION.cyberbullying.value_counts()
dataset_Notcontains_MENTION.plot(kind='bar', color=["red", "blue"], ax=ax[1])
ax[1].set_xticklabels(('cyberbullying', 'Not cyberbullying'), rotation=0)
ax[1].set_title('Does not contain "MENTION"')

# Show the plots
plt.tight_layout()
plt.show()




dataset_contains_mkr = dataset_rus.loc[dataset_rus["tweet_text"].str.lower().str.contains("mkr")]
dataset_Notcontains_mkr= dataset_rus.loc[~dataset_rus["tweet_text"].str.lower().str.contains("mkr")]

fig, axs = plt.subplots(1, 2, figsize=(15, 6))


etiquetas_contains_mkr = dataset_contains_mkr.cyberbullying.value_counts()
etiquetas_contains_mkr.plot(kind='bar', color=["red", "blue"], ax=axs[0])
axs[0].set_xticklabels(('cyberbullying', 'Not cyberbullying'), rotation=0)
axs[0].set_title('Contains "mkr"')

etiquetas_Notcontains_mkr = dataset_Notcontains_mkr.cyberbullying.value_counts()
etiquetas_Notcontains_mkr.plot(kind='bar', color=["red", "blue"], ax=axs[1])
axs[1].set_xticklabels(('cyberbullying', 'Not cyberbullying'), rotation=0)
axs[1].set_title('Does not contain "mkr"')

# Show the plots
plt.tight_layout()
plt.show()




dataset_contains_bully = dataset_rus.loc[dataset_rus["tweet_lemma_NoStopwordNoPunct"].str.contains("bully")]
dataset_Notcontains_bully = dataset_rus.loc[~dataset_rus["tweet_lemma_NoStopwordNoPunct"].str.contains("bully")]

fig, ax = plt.subplots(1, 2, figsize=(15, 6))


dataset_contains_bully = dataset_contains_bully .cyberbullying.value_counts()
dataset_contains_bully .plot(kind='bar', color=["red", "blue"], ax=ax[0])
ax[0].set_xticklabels(('cyberbullying', 'Not cyberbullying'), rotation=0)
ax[0].set_title('Contains "bully "')

dataset_Notcontains_bully  = dataset_Notcontains_bully.cyberbullying.value_counts()
dataset_Notcontains_bully.plot(kind='bar', color=["red", "blue"], ax=ax[1])
ax[1].set_xticklabels(('cyberbullying', 'Not cyberbullying'), rotation=0)
ax[1].set_title('Does not contain "bully "')

# Show the plots
plt.tight_layout()
plt.show()










from profanity_check import predict, predict_prob


print(dataset.tweet_text[0])
print(predict_prob([dataset.tweet_text[0]]))
predict_prob(["idiot"])


dataset["profanity_prob"] = dataset.tweet_text.apply(lambda x: predict_prob([x])[0])
dataset["profanity_prob"]





def relation_word_cyberbullying(dataset,palabra,column="tweet_lemma_NoStopwordNoPunct"):
    dataset_contains_MENTION = dataset.loc[dataset[column].str.contains(palabra)]
    dataset_Notcontains_MENTION= dataset.loc[~dataset[column].str.contains(palabra)]
    
    fig, ax = plt.subplots(1, 2, figsize=(15, 6))
    
    
    dataset_contains_MENTION = dataset_contains_MENTION.cyberbullying.value_counts()
    dataset_contains_MENTION.plot(kind='bar', color=["red", "blue"], ax=ax[0])
    ax[0].set_xticklabels(('cyberbullying', 'Not cyberbullying'), rotation=0)
    ax[0].set_title(f"Contains {palabra}")
    
    dataset_Notcontains_MENTION = dataset_Notcontains_MENTION.cyberbullying.value_counts()
    dataset_Notcontains_MENTION.plot(kind='bar', color=["red", "blue"], ax=ax[1])
    ax[1].set_xticklabels(('cyberbullying', 'Not cyberbullying'), rotation=0)
    ax[1].set_title(f"Does not contain {palabra}")
    
    # Show the plots
    plt.tight_layout()
    plt.show()



relation_word_cyberbullying(dataset_rus,"URL")
plt.show()
# No encontré nada muy significativo a mano más allá de las previamente mencionadas y URL





dataset["entities"] = dataset.tweet_text_doc.apply(lambda x: [x for x in list(x.ents)] )
dataset["entity_labes"] = dataset.tweet_text_doc.apply(lambda x: [x.label_ for x in list(x.ents)])
dataset["amount_entities"] = dataset.entity_labes.apply(lambda x: len(x))





dataset["amount_url"] =dataset.tweet_text.apply(lambda x:count_slashes(x,"URL")) 
dataset["amount_mention"] =dataset.tweet_text.apply(lambda x:count_slashes(x,"MENTION")) 
dataset["amount_number"] =dataset.tweet_text.apply(lambda x:count_slashes(x,"NUMBER")) 
dataset["amount_hashtag"] =dataset.tweet_text.apply(lambda x:count_slashes(x,"HASHTAG")) 
dataset["amount_mkr"] =dataset.tweet_text.apply(lambda x:count_slashes(x,"mkr")) 
dataset["amount_bully"] =dataset.tweet_text.apply(lambda x:count_slashes(x,"bully")) 



jd.grafica_relacion_cat_obj(dataset, "amount_entities","cyberbullying", to_words=True,order=True, dame_chi_cuadrado = True,size=(30,10))



dataset["More_ThanThreeEntities"] = dataset.amount_entities.apply(lambda x: 0 if x <=3 else 1)

jd.grafica_relacion_cat_obj(dataset, "More_ThanThreeEntities","cyberbullying",to_words = True,
                            dame_chi_cuadrado=True, size=(20,6))







"""
Intento fallido, devuelve lista de sets que son todos valores únicos

dataset_cyberbullying1 = dataset[dataset["cyberbullying"]==1]
dataset_cyberbullying0 = dataset[dataset["cyberbullying"]==0]

ax, fig = plt.subplots(figsize=(20,14))
dataset_cyberbullying1[dataset_cyberbullying1["entities"].str.len() !=0].entities.value_counts().head(60).plot(kind="barh")
plt.title("Frecuencia de entidades en tweets con bullying",size=20)
plt.xlabel("Frecuencia",size = 15)
plt.ylabel("Entidades",size = 15)
"""





#nltk.download('vader_lexicon')


dataset.columns


import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid =SentimentIntensityAnalyzer()

dataset["negative_sentiment_score"] = dataset.tweet_text.apply(lambda x: sid.polarity_scores(x)["neg"] )
dataset["positive_sentiment_score"] = dataset.tweet_text.apply(lambda x: sid.polarity_scores(x)["pos"] )
dataset["neutral_sentiment_score"] = dataset.tweet_text.apply(lambda x: sid.polarity_scores(x)["neu"] )
dataset["neutral_sentiment_score"] = dataset.tweet_text.apply(lambda x: sid.polarity_scores(x)["compound"] )





from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

mat=jd.get_cat_corr_mat(dataset)
#sns.heatmap(mat.to_numpy(),annot =True)

disp = ConfusionMatrixDisplay(confusion_matrix=mat.to_numpy(),
                              display_labels=mat.select_dtypes(exclude = "object").columns)

                                              
fig, ax = plt.subplots(figsize=(30, 10))
disp.plot(ax=ax)
plt.xticks(rotation=90)
ax.grid(False)
plt.show()





dataset.drop(["amount_bully","amount_misspelled_words","amount_entities","entities",
              "entity_labes"],axis=1,inplace=True)





plt.figure(figsize=(20,6))
mat=jd.get_cat_corr_mat(dataset)
sns.heatmap(mat,cmap="coolwarm",annot = True)


import gensim.downloader as api
glove_emb = api.load('glove-twitter-25') # Descargamos y cargamosel embedding de "glove-twitter-25"


from nltk.tokenize import TweetTokenizer
# Tokenizar los tweets con el tokenizador "TweetTokenizer" de NLTK
def tokenize(texto):
  tweet_tokenizer = TweetTokenizer()
  tokens_list = tweet_tokenizer.tokenize(texto)
  return tokens_list


def get_average_vector(sentence):
  #retokenizamos con nuestra función
  tokens = tokenize(sentence)
  lista = list()
  for i in tokens:
    try:
      lista.append(glove_emb.get_vector(i) )
    except:
      continue

  try:
    resultado = np.mean(lista, axis=0)
  except:
    resultado = np.zeros(25)
  return resultado


dataset["embeddings"] = dataset["tweet_lemma_NoStopwordNoPunct"].apply(lambda x: get_average_vector(x))


dataset.embeddings.apply(pd.Series)


dataset.select_dtypes(include = np.number).columns


dataset.select_dtypes(exclude = np.number).columns


vector_data = pd.concat([dataset.embeddings.apply(pd.Series),
                dataset.select_dtypes(include = np.number)], axis=1)

vector_data


vector_data = vector_data.fillna(0)
import scipy as sp

y = vector_data["cyberbullying"].values.astype(np.float32)
X = sp.sparse.csc_matrix(vector_data.drop("cyberbullying",axis=1))


vector_data["cyberbullying"].value_counts().plot(kind="bar",figsize=(16,6))
plt.title("Variable objetivo demasiado desbalanceada, necesitaremos usar alguna técnica de remuestreo",size= 20)
plt.show()


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,test_size=0.25,stratify=y)
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)





from imblearn.combine import SMOTETomek

smote_tomek = SMOTETomek(random_state=0)
X_res, y_res = smote_tomek.fit_resample(X_train.toarray(), y_train)



from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

classifiers = {
    'Naive Bayes': GaussianNB(),
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(n_jobs=-1)
}

# Train and evaluate classifiers
for name, classifier in classifiers.items():
    classifier.fit(X_res, y_res)
    
    cv_scores = cross_val_score(classifier, X_res, y_res, cv=4,scoring = "accuracy")
    print(f"Train Cross-validation mean Accuracy for {name}: {round(cv_scores.mean(),4)}")
    
    y_pred = classifier.predict(X_test.toarray())
    print(f"Classification Report on Test set for {name}:")
    print(classification_report(y_test, y_pred))
    
    print(f"Confusion Matrix on Test set for {name}:")
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)
    disp.plot()
    plt.title(f'Confusion Matrix for {name}')
    plt.show()






#### Después de un tiempo de espera excesivamente largo, consigo un random forest ligeramente peor


"""
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
#Best Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'n_estimators': 150}

rf_classifier = RandomForestClassifier()


param_grid = {
    "criterion": ["gini", "entropy", "log_loss"],
    'n_estimators': [100, 150],
    'max_depth': [None, 20],
    "max_features": ["sqrt", "log2", None],
}


grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=4, n_jobs=-1,verbose=1)
grid_search.fit(X_res.toarray(), y_res)

print("Best Parameters:", grid_search.best_params_)


y_pred = grid_search.predict(X_test.toarray())
print("Classification Report on test for best random forest found:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=grid_search.classes_)
disp.plot()
plt.title('Confusion Matrix on test for best random forest found:')
plt.show()
"""





rf_classifier = RandomForestClassifier(n_jobs=-1)# Lo vuelvo a instanciar sin parámetros
rf_classifier.fit(X_res, y_res)


import shap 
X_res_summary = shap.kmeans(X_res, 100)
explainer = shap.KernelExplainer(rf_classifier.predict,X_res_summary)


shap_values = explainer.shap_values(X_test[0:100])


shap.summary_plot(shap_values, X_test[0:100],feature_names=vector_data.drop("cyberbullying",axis=1).columns,
                  color=plt.get_cmap("cool"))

# SHAP tiene muchos fallos, este se tiene desde 2019 reportado "https://github.com/shap/shap/issues/406"
# en el github dice el creador que se solucionó el problema, pero en 2021 este usuario de stackoverflow no recibió respuesta:
#https://stackoverflow.com/questions/68620866/why-is-my-shap-plot-in-python-returning-grey-instead-of-red-and-blue

# Se puede ver feature importance, pero no si afecta positiva o negativamente. Parece que las features que creé influyen mucho al modelo.






from imblearn.under_sampling import RandomUnderSampler,NearMiss,TomekLinks, EditedNearestNeighbours
from imblearn.over_sampling import SMOTE, ADASYN,BorderlineSMOTE,SVMSMOTE, KMeansSMOTE
from imblearn.combine import SMOTETomek
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score

def muchos_train_test_resampled(X,y,modelo,n=20, resampling="SMOTETomek",seed=123,title=False,sampling_strategy="auto",tipo = "True_PosNeg_Rate"):
    """
    Para que no haya confusión, así defino la matriz:  [[TP,FN],
                                                        [FP, TN]]   
    True rate: de todos los true values cuántos identifico.
    """
    rng = np.random.default_rng(seed)
    TP= 0 
    FP = 0
    FN = 0
    TN = 0 
    TP_rate = []
    TN_rate = []
    Accuracy = []
    F1_Score = []
    if resampling== "SMOTETomek":
        resampler = SMOTETomek(random_state=rng.integers(1000000),sampling_strategy = sampling_strategy)
    elif resampling== "RandomUnderSampler":
        resampler = RandomUnderSampler(random_state=rng.integers(1000000),sampling_strategy = sampling_strategy)
    elif resampling== "ADASYN":
        resampler = ADASYN(random_state=rng.integers(1000000),sampling_strategy = sampling_strategy)
    elif resampling== "SMOTE":
        resampler = SMOTE(random_state=rng.integers(1000000),sampling_strategy = sampling_strategy)
    elif resampling== "BorderlineSMOTE":
        resampler = BorderlineSMOTE(random_state=rng.integers(1000000),sampling_strategy = sampling_strategy)
    elif resampling== "SVMSMOTE":
        resampler = SVMSMOTE(random_state=rng.integers(1000000),sampling_strategy = sampling_strategy)
    elif resampling== "KMeansSMOTE":
        resampler = KMeansSMOTE(random_state=rng.integers(1000000),sampling_strategy = sampling_strategy)
    elif resampling== "NearMiss":
        resampler = NearMiss(version=2)
    elif resampling== "TomekLinks":
        resampler = TomekLinks(sampling_strategy = "majority")
    elif resampling== "EditedNearestNeighbours":
        resampler = EditedNearestNeighbours(random_state=rng.integers(1000000))
   
    for i in range(n):
        seed = rng.integers(1000000)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = seed, test_size=0.2,stratify=y)
        X_train_st, y_train_st = resampler.fit_resample(X_train, y_train)
        modelo.fit(X_train_st,y_train_st)
        y_pred = modelo.predict(X_test)
        matriz_confusion = confusion_matrix(y_test, y_pred)

        TP += matriz_confusion[0][0]
        TP_rate_iteracion = (matriz_confusion[0][0])/(matriz_confusion[0][0]+ matriz_confusion[0][1])
        TP_rate.append(TP_rate_iteracion)
        FP += matriz_confusion[0][1]
        FN += matriz_confusion[1][0]
        TN += matriz_confusion[1][1]
        TN_rate_iteracion = (matriz_confusion[1][1])/(matriz_confusion[1][1]+ matriz_confusion[1][0])
        TN_rate.append(TN_rate_iteracion)

        Accuracy.append(accuracy_score(y_true=y_test,y_pred=y_pred))
        F1_Score.append(f1_score(y_true=y_test,y_pred=y_pred,pos_label=0))

    TP /= n
    FP /= n
    FN /= n
    TN /= n  

    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize = (30,6))
    fig.suptitle(f"{type(modelo).__name__}+ {resampling} k_fold {n}",fontsize = 20)

    mat=np.array([[int(TP),int(FP)],[int(FN),int(TN)]])
    ax1.set_title("Matriz de confusión", fontsize=30)
    sns.heatmap(mat,cmap="coolwarm",annot=True, fmt='g',ax=ax1)

    if tipo == "True_PosNeg_Rate":
        #ax2
        ax2.set_title("Sesgo-Varianza del True Positive Rate", fontsize=25)
        mean1 = pd.Series(TP_rate).mean()
        std_dev1 = pd.Series(TP_rate).std()
        ax2.annotate(f"Valor medio: {mean1:.4f}", xy=(0.5, 0.96), xycoords='axes fraction', ha='center', fontsize=16)
        ax2.annotate(f"Desviación estándar: {std_dev1:.4f}", xy=(0.5, 0.90), xycoords='axes fraction', ha='center', fontsize=16)
        sns.violinplot(pd.Series(TP_rate),orient="v",inner="box", width=0.65,ax=ax2)
        #ax3
        ax3.set_title("Sesgo-Varianza del True Negative Rate", fontsize=25)
        mean2 = pd.Series(TN_rate).mean()
        std_dev2 = pd.Series(TN_rate).std()
        ax3.annotate(f"Valor medio: {mean2:.4f}", xy=(0.5, 0.96), xycoords='axes fraction', ha='center', fontsize=16)
        ax3.annotate(f"Desviación estándar: {std_dev2:.4f}", xy=(0.5, 0.90), xycoords='axes fraction', ha='center', fontsize=16)
        sns.violinplot(pd.Series(TN_rate),orient="v",inner="box", width=0.65,ax=ax3)


    elif tipo == "Accuracy_F1":
        #ax2
        ax2.set_title("Sesgo-Varianza del Accuracy", fontsize=25)
        mean1 = pd.Series(Accuracy).mean()
        std_dev1 = pd.Series(Accuracy).std()
        ax2.annotate(f"Valor medio: {mean1:.4f}", xy=(0.5, 0.96), xycoords='axes fraction', ha='center', fontsize=16)
        ax2.annotate(f"Desviación estándar: {std_dev1:.4f}", xy=(0.5, 0.90), xycoords='axes fraction', ha='center', fontsize=16)
        sns.violinplot(pd.Series(Accuracy),orient="v",inner="box", width=0.65,ax=ax2)
        #ax3
        ax3.set_title("Sesgo-Varianza del F1 Score", fontsize=25)
        mean2 = pd.Series(F1_Score).mean()
        std_dev2 = pd.Series(F1_Score).std()
        ax3.annotate(f"Valor medio: {mean2:.4f}", xy=(0.5, 0.96), xycoords='axes fraction', ha='center', fontsize=16)
        ax3.annotate(f"Desviación estándar: {std_dev2:.4f}", xy=(0.5, 0.90), xycoords='axes fraction', ha='center', fontsize=16)
        sns.violinplot(pd.Series(F1_Score),orient="v",inner="box", width=0.65,ax=ax3)

    plt.show()

rf_classifier = RandomForestClassifier(n_jobs=-1)


muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="SMOTETomek",seed=123)
plt.show()


muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="SMOTE",seed=123)
plt.show()


muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="BorderlineSMOTE")
plt.show()



muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="RandomUnderSampler",seed=123)
plt.show()


muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="ADASYN",seed=123)
plt.show()


muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="SVMSMOTE",seed=123)
plt.show()


muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="NearMiss",seed=123)
plt.show()


muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="TomekLinks",seed=123)
plt.show()





for i in [1,0.8,0.6,0.4]:
    print(i)
    muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="SMOTE",seed=123,
                                sampling_strategy=i,tipo="Accuracy_F1")
    plt.show()





for i in [0.95,0.90,0.85]:
    print(i)
    muchos_train_test_resampled(X.toarray(),y,rf_classifier,n=10, resampling="SMOTE",seed=123,
                                sampling_strategy=i,tipo="Accuracy_F1")
    plt.show()












